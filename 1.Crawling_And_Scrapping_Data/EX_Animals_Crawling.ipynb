{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8279d78e",
   "metadata": {},
   "source": [
    "<font size=\"10\">\n",
    "    <b>\n",
    "        Wikipedia Extinct Species Crawling \n",
    "        <b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8a367",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    <b>\n",
    "        Opening the downloaded site page \n",
    "        <b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af446c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_EX_Holocene = \"https://en.wikipedia.org/wiki/Timeline_of_extinctions_in_the_Holocene\"\n",
    "param = \"./Data/\"\n",
    "site_EX = \"Timeline of extinctions in the Holocene - Wikipedia.html\"\n",
    "soup = BeautifulSoup(open(param + site_EX, encoding=\"utf8\"), \"html.parser\")\n",
    "table_tag = soup.find_all(\"table\",attrs={\"class\":\"wikitable sortable jquery-tablesorter\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14cad8",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    <b>\n",
    "        Function to crawl each link and get the main animal page data\n",
    "        <b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDfByCrawlIngWiki(link):\n",
    "    \n",
    "    temp=0\n",
    "    property_names = list()\n",
    "    property_values = list()\n",
    "    html = requests.get(link)\n",
    "    soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "\n",
    "    tbl=soup.find(\"table\",attrs={\"class\":\"infobox biota\"})\n",
    "    if tbl != None:\n",
    "        for row in tbl(\"tr\"):\n",
    "            for link in row.find_all('td'):\n",
    "                if(len(row.find_all('td'))==2):\n",
    "                    if temp==0:\n",
    "                        property_names.append(link.get_text().strip())\n",
    "                        temp=1\n",
    "                    else:\n",
    "                        if link.get_text().strip() == \"Plantae\":\n",
    "                            df_range = len(property_names)\n",
    "                            Extinct_Animals_df = pd.DataFrame([range(df_range)])\n",
    "                            Extinct_Animals_df.columns = property_names\n",
    "                            return Extinct_Animals_df\n",
    "                        property_values.append(link.get_text().strip())\n",
    "                        temp=0\n",
    "\n",
    "        df_range = len(property_names)\n",
    "        Extinct_Animals_df = pd.DataFrame([range(df_range)])\n",
    "        Extinct_Animals_df.columns = property_names\n",
    "        Extinct_Animals_df.iloc[0] = property_values\n",
    "\n",
    "        return Extinct_Animals_df\n",
    "    else:\n",
    "        df_range = len(property_names)\n",
    "        Extinct_Animals_df = pd.DataFrame([range(df_range)])\n",
    "        Extinct_Animals_df.columns = property_names\n",
    "        \n",
    "        return Extinct_Animals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c2b64",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    <b>\n",
    "        Crawling the tables of the main page and entering each animal link for further data scrapping\n",
    "        <b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53799b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting main page data and calling link_crawl#\n",
    "\n",
    "linksOfAnimalPages=list()\n",
    "df_end = pd.DataFrame([range(1)])\n",
    "isStart = True\n",
    "count = 0\n",
    "\n",
    "for table in table_tag:\n",
    "    #table = soup.find(\"table\")\n",
    "    thread_tag = table.find(\"thead\")\n",
    "    body_tag = table.find(\"tbody\")\n",
    "    property_rows = thread_tag.find_all(\"th\")\n",
    "    animal_rows = body_tag.find_all(\"tr\")\n",
    "    listOfProperties=list()\n",
    "    listOfAnimalData=list()\n",
    "    \n",
    "    for row in property_rows:\n",
    "            listOfProperties.append(row.get_text(strip=True))\n",
    "    df_range = len(listOfProperties)\n",
    "    df = pd.DataFrame([range(df_range)])\n",
    "    df.columns = listOfProperties\n",
    "    dfOfAnimal = df\n",
    "\n",
    "    for animal_row in body_tag: #foreach tr in current table\n",
    "        animalDataCount = 0\n",
    "        listOfAnimalData=list()\n",
    "        a_tag = animal_row.find(\"a\")\n",
    "        \n",
    "        if a_tag != -1:\n",
    "            linksOfAnimalPages.append(a_tag[\"href\"])\n",
    "            listOfAnimalData.append(a_tag.get_text(strip=True))\n",
    "            \n",
    "            df_func = MakeDfByCrawlIngWiki(a_tag[\"href\"])\n",
    "            \n",
    "        i_tag = animal_row.find(\"i\")\n",
    "        if i_tag != -1:\n",
    "            listOfAnimalData.append(i_tag.get_text(strip=True))\n",
    "        for td_row in animal_row:\n",
    "            animalDataCount += 1\n",
    "            if(animalDataCount == 6):\n",
    "                listOfAnimalData.append(td_row.get_text(strip=True))\n",
    "            if(animalDataCount == 8):\n",
    "                b_tag = animal_row.find(\"b\")\n",
    "                if b_tag != -1 and b_tag != None:\n",
    "                    listOfAnimalData.append(b_tag.get_text(strip=True))\n",
    "            if(animalDataCount == 10):\n",
    "                if(len(listOfProperties) != 6):\n",
    "                    listOfAnimalData.append(td_row.get_text(strip=True))\n",
    "                    dfOfAnimal.loc[0] = listOfAnimalData\n",
    "                    df = df.append(dfOfAnimal)\n",
    "                else:\n",
    "                    listOfAnimalData.append(td_row.get_text(strip=True))\n",
    "            if(animalDataCount == 12):\n",
    "                listOfAnimalData.append(td_row.get_text(strip=True))\n",
    "                if(dfOfAnimal.shape[1] == len(listOfAnimalData)):\n",
    "                    dfOfAnimal.loc[0] = listOfAnimalData\n",
    "                    df = df.append([dfOfAnimal])\n",
    "        \n",
    "    df = pd.concat([df,df_func], axis=1)                    \n",
    "    if(isStart):\n",
    "        df_end = df\n",
    "        isStart = False\n",
    "    else:\n",
    "        df_end = df_end.append([df])\n",
    "        \n",
    "#df_end.to_csv('C:/Users/noypr/Downloads/ExAnimals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
